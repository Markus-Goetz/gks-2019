{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Logistic regression is a simple machine learning algorithm for classification. It computes the weighted sum of its inputs and outputs an *activation* that turns the weighted sum into fixed interval (here: [0,1]). This allows us to interpret the logistic regression output as the probability of being of a class or not.\n",
    "In this toy example we are going to use it to decide, to which of two distributions a data point belongs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "For a detailed explanation of the used modules, please refer to the respective sections in the [introductory notebook](0_MNIST_dataset.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(842424)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation of Artificial Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we generate some artificial data for our toy model to learn on.\n",
    "The task is to predict to which normal distributions one individual data point belongs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(n_samples=1000, input_dim=2):\n",
    "    \"\"\"\n",
    "    Return coordinates distributed in 2D space by two different gaussian distributions.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    x : np.array[n_samples, input_dim]\n",
    "        coordinates\n",
    "    y : np.array[n_samples]\n",
    "        labels\n",
    "    \"\"\"\n",
    "    half_samples = n_samples // 2\n",
    "    \n",
    "    # generate the blobs\n",
    "    x1 = np.random.normal(1., 0.25, size=(half_samples, input_dim))\n",
    "    x2 = np.random.normal(2., 0.30, size=(half_samples, input_dim))\n",
    "    \n",
    "    # create matching labels\n",
    "    y1 = np.zeros(half_samples)\n",
    "    y2 = np.ones(half_samples)\n",
    "    \n",
    "    return np.concatenate((x1, x2)), np.concatenate((y1, y2))\n",
    "\n",
    "data, labels = generate_data()\n",
    "data.shape, labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shuffle the data to improve covergence behaviour and to more closely emulate real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(shuffled_indices)\n",
    "\n",
    "data, labels = data[shuffled_indices], labels[shuffled_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And plot the points colored by label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(data, labels):\n",
    "    \"\"\"\n",
    "    Plot data colored by labels.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : np.array[n_samples, input_dim]\n",
    "    labels : np.array[input_dim]\n",
    "    \"\"\"\n",
    "    plt.scatter(data[:, 0], data[:, 1], c=labels, vmin=0.0, vmax=1.0)\n",
    "    plt.show()\n",
    "    \n",
    "plot_data(data, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Logistic Function\n",
    "The logistic (or sigmoid) function gives the predicted probability of the tested hypothesis being true.\n",
    "In our case: does the input belong to the yellow distribution?\n",
    "\n",
    "$$\\sigma(z) = \\frac{1}{1 + \\text{e}^{-z}}$$\n",
    "\n",
    "The activation function of the logistic regression is applied to the dot product of the weights of the model and the input.\n",
    "To optimize the model with a gradient descent optimizer, the activation function has to be differentiable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"Return the sigmoid of input z.\"\"\"\n",
    "    ##############\n",
    "    return the sigmoid of z\n",
    "    ##############\n",
    "\n",
    "def plot_sigmoid():\n",
    "    \"\"\"\n",
    "    Plot sigmoid function from -10 to 10.\n",
    "    \"\"\"\n",
    "    plot_range = np.arange(-10, 10, 0.01)\n",
    "    ##############\n",
    "    plt the sigmoid in the given range\n",
    "    ##############\n",
    "    plt.show()\n",
    "    \n",
    "plot_sigmoid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Model\n",
    "\n",
    "The logistic regression is a very simple linear model. It encodes a single straight line (in 2D). Everything beyond the line does not belong to the class.\n",
    "\n",
    "In the multiclass case, one simply trains multiple models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_predict(weights, x):\n",
    "    \"\"\"\n",
    "    Return the prediction of the model for input x with the given weights.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    weights: the weights of the model to be learned. There is one weight for every input dimension plus a bias.\n",
    "    x: input data where the 0th input should be 1.0 to take the bias into account in a simple dot product.\n",
    "    \n",
    "    Returns:\n",
    "    ----------\n",
    "    The activation of the logistic regression, the sigmoid of the dot product of weights and input.\n",
    "    \"\"\"\n",
    "    ##############\n",
    "    return the prediction for x\n",
    "\n",
    "    weights = initialize the weights and dont forget the bias\n",
    "    ##############\n",
    "    \n",
    "ones = np.ones((data.shape[0], 1,))\n",
    "plot_data(data, lr_predict(weights, np.hstack([ones, data])))\n",
    "plot_data(data, np.around(lr_predict(weights, np.hstack([ones, data]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Loss Function\n",
    "\n",
    "The loss function measures how well the model performs. If the model predicts all samples correctly it would be 0.\n",
    "Here we use Mean Squared Error:\n",
    "\n",
    "$$ MSE(\\textrm{samples}) = \\frac{1}{2 |\\textrm{samples}|} \\sum_{x \\in \\textrm{samples}} \\left(\\hat{y}(x) - y_{\\textrm{label}}\\right)^2$$\n",
    "\n",
    "To minimize the loss, we also compute the gradient w.r.t. a single weight.\n",
    "\n",
    "$$ \\nabla = (\\hat{y}(x) - y_{\\textrm{label}}) \\cdot y(x)^2 \\cdot \\textrm{e}^{-w \\cdot x} \\cdot x$$\n",
    "\n",
    "where the weight vector is $w$ with the bias $w_0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_loss(weights, x, label):\n",
    "    \"\"\"Return the loss and the gradient with respect to the weights.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    weights : weights of the model to be learned, where weights[0] is the bias\n",
    "    x : single input sample, x[0] is assumed to be 1\n",
    "    label : label belonging to x\n",
    "    \n",
    "    Returns\n",
    "    -----------\n",
    "    loss : mean square error loss for a single sample\n",
    "    gradient : np.array([weights.shape]) \n",
    "        gradient of loss with respect to labels\n",
    "    \"\"\"\n",
    "    loss = 0.\n",
    "    features = x.shape[0]\n",
    "    \n",
    "    ##############\n",
    "    y = compute the prediction for x\n",
    "    \n",
    "    loss = compute the loss for y\n",
    "    gradient = compute the gradient of the loss\n",
    "    ##############\n",
    "    return loss, gradient\n",
    "\n",
    "lr_loss(weights, np.concatenate([[1], data[0]]), labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model Manually\n",
    "To train the model, we first initialize the weights (plus bias) and compute the corresponding loss.\n",
    "To comfortably incorporate the bias, prepend $x_0 = 1$ to the input data so you can just compute the dot product in the functions above.\n",
    "We then update the weights with the gradient of the loss, until the model converges.\n",
    "Since loss is not a very intuitable quantity, we also print the accuracy (percentage of correctly classified samples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_train(weights, x, labels, epochs=100, lr=0.001):\n",
    "    \"\"\"\n",
    "    Train the model, i.e. update the weights following the negative gradient until the model converges.\n",
    "    \"\"\"\n",
    "    n_samples = x.shape[0]\n",
    "    ones = np.ones((n_samples, 1,))\n",
    "    x = np.hstack([ones, x])\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        ##############\n",
    "        compute loss and accuracy and print them during the training\n",
    "        \n",
    "        for n in range(n_samples):\n",
    "            compute loss and gradient\n",
    "            update weights\n",
    "            print training status\n",
    "        ##############\n",
    "    return weights\n",
    "\n",
    "weights = lr_train(weights, data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ones = np.ones((data.shape[0], 1,))\n",
    "plot_data(data, lr_predict(weights, np.hstack([ones, data])))\n",
    "plot_data(data, np.around(lr_predict(weights, np.hstack([ones, data]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras\n",
    "Building a model can often be simplified with a high level API like Keras.\n",
    "Our simple model in the Keras vocabulary is defined as a $\\texttt{Sequential}$ model with a single $\\texttt{Dense}$ layer \n",
    "with a sigmoid activation function, $\\texttt{mse}$ loss and a stochastic gradient descent ($\\texttt{SGD}$) optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest model in Keras is $\\texttt{Sequential}$, a linear stack of layers. The layers can be passed to the constructor as a list, or $\\texttt{add}$ed later.\n",
    "\n",
    "The $\\texttt{Dense}$ layer encodes:\n",
    "$$ f(W \\cdot x)$$\n",
    "where $W \\in \\mathbb{R}^{\\text{m x 1+n}}$, x = (1, $\\textrm{input}_1$, .. , $\\textrm{input}_{\\textrm{n}}$) and f applies the activation function to each element in the resulting vector.\n",
    "$m$ and $n$ are output- and input dimensions respectively ($m = 1$ in this case).\n",
    "In short: every output is connected to every input plus a bias.\n",
    "\n",
    "Keras also always uses minibatches by default. Why?\n",
    "\n",
    "It is then \"compiled\" with a stochastic gradient descent optimizer and an mean square error loss.\n",
    "These parameters can be given by keyword or custom objects. E.g. here the default learning rate of the SGD optimizer is too slow for this simple demonstration so we supply a custom $\\texttt{optimizer}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(data):\n",
    "    \"\"\"\n",
    "    Build a simple perceptron for the given input data.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1, input_dim=data.shape[1], activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer=SGD(lr=0.05), loss='mse', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = build_model(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training is done with the $\\texttt{fit}$ method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, data, labels, epochs=100, batch_size=50):\n",
    "    \"\"\"\n",
    "    Train the given model with the given data and labels for the given epochs.\n",
    "    \"\"\"\n",
    "    model.fit(data, labels, batch_size=batch_size, epochs=epochs)\n",
    "    \n",
    "train_model(model, data, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows the prediction of the network in a more systematic way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction_grid(model, grid_size=30):\n",
    "    \"\"\"\n",
    "    Plot a 2D grid and the network prediction at every grid point for the given grid size.\n",
    "    \"\"\"\n",
    "    x_coord = np.zeros((grid_size, grid_size))\n",
    "    y_coord = np.zeros((grid_size, grid_size))\n",
    "    \n",
    "    for i in range(grid_size):\n",
    "        for j in range(grid_size):\n",
    "            x_coord[i, j] = i / 10.0\n",
    "            y_coord[i, j] = j / 10.0\n",
    "            \n",
    "    x_coord = x_coord.reshape(-1, 1)\n",
    "    y_coord = y_coord.reshape(-1, 1)    \n",
    "    \n",
    "    prediction = model.predict(np.hstack([x_coord, y_coord]))\n",
    "    plt.scatter(x_coord.flatten(), y_coord.flatten(), c=prediction.flatten())\n",
    "    plt.show()\n",
    "\n",
    "plot_prediction_grid(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(data, model.predict(data).flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus:\n",
    "Why is a simple logistic regression not the optimal solution for this task?\n",
    "\n",
    "What is the property required of an actual model solution?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
